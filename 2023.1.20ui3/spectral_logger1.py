## A truncated version of spectral_logger in VOC fitter
from pathlib import Path
import collections
import h5py
import numpy as np
from typing import List
import pandas as pd


# Used for parsing the names of h5 files
FITNAME_OFFSET = -3
COUNTER_OFFSET = -1
HOSTNAME_OFFSET = 1
def sort_by_timestamp_then_count(path_obj):
    stem = path_obj.stem
    timestamp = int("".join(stem.split('_')[-3:-1]))
    count = int(stem.split('_')[-1])
    return timestamp, count

class SpectralLogReader:
    """
    Reader class for spectral files generated by SpectralLog class

    Args:
        source_dir (path): location of files.  Class will drill down into subdirectories
    """
    def __init__(self, source_dir, verbose=False):
        source_dir = Path(source_dir)
        files = sorted(source_dir.rglob("*.h5"))
        self.file_list = collections.defaultdict(list)
        self.session_list = dict()  # Maintains a map of fit names to session indexes
        self.session_limits = dict()  # Stores the maximum session index
        self.all_row_ids = dict()  # Map of fitname to all row IDs across active sessions
        self.session_array = dict()  # Stores an index of global row ID to session, for each fitname
        self.selected_files = dict()  # Mapping from session-row to files, for each fitname
        self.unique_spectrum_id = dict()  # Global row ID for active session
        self.files_by_timestamp =  dict()  # Map of fitname-timestamps to files
        self.verbose = verbose
        self.timestamps = None
        self._process_directories(files)
        self._organize_spectra()

    def _process_directories(self, h5_files):
        # Performs initial cataloguing of log files beneath the root directory
        for fpath in h5_files:
            name = fpath.stem
            namegroups = name.split('_')
            fitname = '_'.join(namegroups[HOSTNAME_OFFSET:FITNAME_OFFSET])
            self.file_list[fitname].append(fpath)

        for fitname in self.file_list:
            self.file_list[fitname].sort(key=sort_by_timestamp_then_count)

    def _get_file_list(self, fitname, session_index=0):
        # Helper function for indexing into logging sessions and retrieving all corresponding files
        if fitname not in self.fit_names:
            raise KeyError(f"Fitter {fitname} not found. Available fits: {list(self.file_list.keys())}")
        timestamp_index = self.timestamps[fitname][session_index]
        file_list = self.files_by_timestamp[fitname][timestamp_index]
        return file_list

    def _organize_spectra(self):
        # Organizes results by unique timestamp in file name.
        self.files_by_timestamp = collections.defaultdict(dict)
        self.timestamps = collections.defaultdict(list)
        for fitname, files in self.file_list.items():
            files = self.file_list[fitname]
            if self.verbose: print(f'processing {fitname} files...', end=': ')
            count = 0
            for fpath in files:
                # Extract the timestamp from the file name. This corresponds to a logging "session"
                timestamp = int("".join(fpath.stem.split('_')[FITNAME_OFFSET:COUNTER_OFFSET]))
                self.timestamps[fitname].append(timestamp)
                if timestamp not in self.files_by_timestamp[fitname]:
                    self.files_by_timestamp[fitname][timestamp] = [fpath]
                else:
                    self.files_by_timestamp[fitname][timestamp].append(fpath)
                count += 1
            if count == 0 and self.verbose:
                print('no spectra found')
            elif self.verbose:
                print(f'{count} spectra found')

        # Use the timestamps attribute to index into different logging sessions. Remove duplicates.
        for fitname in self.fit_names:
            self.timestamps[fitname] = sorted(set(self.timestamps[fitname]))
            self.session_list[fitname] = list(range(len(self.timestamps[fitname])))
            self.session_limits[fitname] = len(self.timestamps[fitname])

        # Sort the file inside each fitname-timestamp bucket
        sort_by_count = lambda x: int(x.stem.split('_')[-1])
        for fitname in self.fit_names:
            for timestamp in self.files_by_timestamp[fitname]:
                unsorted_filenames = self.files_by_timestamp[fitname][timestamp]
                sorted_filenames = sorted(unsorted_filenames, key=sort_by_count)
                self.files_by_timestamp[fitname][timestamp] = sorted_filenames

        # Set up some data structures used for re-indexing the row IDs across sessions
        for fitname in self.fit_names:
            # Using a numpy array prevents Python from casting as float later on
            self.all_row_ids[fitname] = np.array([], dtype=int)
            self.session_array[fitname] = np.array([], dtype=int)
            self.selected_files[fitname] = {}
            self.set_sessions(fitname, list(range(self.session_limits[fitname])))

    def set_sessions(self, fitname: str, session_list: List[int]):
        """ Set an availability window on the session index, per fitname """
        session_list = sorted(session_list)
        if session_list[-1] > self.session_limits[fitname]:
            raise ValueError(f"Session index for {fitname} is too high. Maximum session index is {self.session_limits[fitname]}")
        self.session_list[fitname] = session_list

    @property
    def fit_names(self):
        """ Returns the list of fit names found in logs beneath the root directory """
        return list(self.file_list.keys())

    def get_result_column_names(self, fitname, session_index=0):
        """ Gets the list of results column names for a given fitter """
        index = 0  # Always choose the first row to sniff out the columns
        file_list = self._get_file_list(fitname, session_index)
        fpath = file_list[index]
        with h5py.File(fpath, 'r') as logfile:
            column_names = list(logfile['results'].keys())
        return column_names

    def print_result_column_names(self, fitname, session_index=0):
        print(self.get_result_column_names(fitname, session_index))

    def get_spectra_column_names(self, fitname, session_index=0):
        """ Gets the list of spectrum column names for a given fitter """
        index = 0  # Always choose the first row to sniff out the columns
        file_list = self._get_file_list(fitname, session_index)
        fpath = file_list[index]
        with h5py.File(fpath, 'r') as logfile:
            spectrum_index = list(logfile['spectra'].keys())[0]  # There may be a better way...
            column_names = list(logfile['spectra'][spectrum_index].keys())
        return column_names

    def print_spectra_column_names(self, fitname, session_index=0):
        print(self.get_spectra_column_names(fitname, session_index))

    def get_column(self, fitname, colname, n_files=-1, session_index=-1):
        """
        Accesses a single column of data from a log session. To access all the rows
        in the log across all sessions, use session_index=-1. To limit the number
        of files used in the query, use the n_files parameter.

        Arguments:
            fitname (str): Fit name in log file
            colname (str): Data column name
            n_files (int): Number of log files to include in query
            session_index (int): Ordinal index of logs to include in query

        Returns:
            numpy.array: 1D numpy array of data from column ``colname``
        """
        if session_index == -1:
            return self._get_column_full(fitname, colname)
        else:
            return self._get_column(fitname, colname, n_files, session_index)

    def _get_column(self, fitname, colname, n_files, session_index):
        filedata = []
        datacount = 0
        file_list = self._get_file_list(fitname, session_index)

        if n_files == -1:  # LBYL
            n_files = len(file_list)
        elif n_files > len(file_list):
            raise ValueError(f"n_files={n_files} too large. Only {len(file_list)} files available "
                             f" for fitter {fitname}.")

        for fpath in file_list[:n_files]:
            with h5py.File(fpath, 'r') as logfile:
                X = logfile['results'][colname][:]
                filedata.append(X)
                datacount += len(X)
        index = 0
        outdata = np.zeros(datacount)
        for data in filedata:
            N = len(data)
            outdata[index:index+N] = data
            index += N
        return outdata

    def _get_columns_full(self, fitname, colnames):
        outdata = {}
        for session_index in self.session_list[fitname]:
            if self.verbose: print(f'loading session index {session_index}')
            session_data = self.get_columns(fitname, colnames, session_index=session_index)
            for key, data in session_data.items():
                if key not in outdata:
                    outdata[key] = data
                else:
                    outdata[key] = np.append(outdata[key], data)
        return outdata

    def get_columns(self, fitname, colnames, n_files=-1, session_index=-1):
        """
        Accesses multiple columns of data from a log session. To access all the rows
        in the log across all sessions, use session_index=-1. To limit the number
        of files used in the query, use the n_files parameter.

        Arguments:
            fitname (str): Fit name in log file
            colnames (list): List of data column name
            n_files (int): Number of log files to include in query
            session_index (int): Ordinal index of logs to include in query

        Returns:
            dict: Dictionary of 1D numpy arrays of data from columns ``colnames``
        """
        if session_index == -1:
            return self._get_columns_full(fitname, colnames)
        else:
            return self._get_columns(fitname, colnames, n_files, session_index)

    def _get_columns(self, fitname, colnames, n_files=-1, session_index=-1):
        filedata = collections.defaultdict(list)
        datacount = collections.defaultdict(int)
        file_list = self._get_file_list(fitname, session_index)

        if n_files == -1:  # LBYL
            n_files = len(file_list)
        elif n_files > len(file_list):
            raise ValueError(f"n_files={n_files} too large. Only {len(file_list)} files available "
                             f" for fitter {fitname}.")

        get_all_columns = True if colnames == '*' else False

        for fi, fpath in enumerate(file_list[:n_files]):
            if fi % 25 == 0 and self.verbose: print(f'{fi} out of {len(file_list)}')
            with h5py.File(fpath, 'r') as logfile:
                if get_all_columns:
                    colnames = list(logfile['results'].keys())
                for colname in colnames:
                    X = logfile['results'][colname][:]
                    filedata[colname].append(X)
                    datacount[colname] += len(X)

        alldata = {}
        for colname in colnames:
            if self.verbose: print(f'processing {colname}...')
            index = 0
            outdata = np.zeros(datacount[colname])
            for data in filedata[colname]:
                N = len(data)
                outdata[index:index+N] = data
                index += N
            alldata[colname] = outdata
        return alldata

    def _get_column_full(self, fitname, colname):
        outdata = np.array([])
        for session_index in self.session_list[fitname]:
            if self.verbose: print(f'loading session index {session_index}')
            outdata = np.append(outdata, self.get_column(fitname, colname, session_index=session_index))
        return outdata

    def results_to_df(self, fitname, session_index=0):
        """
        Accesses a table of data from a log session and stores into a dataframe

        Arguments:
            fitname (str): Fit name in log file
            session_index (int): Ordinal index of logs to include in query

        Returns:
            pandas.DataFrame: data frame of fit results
        """
        file_list = self._get_file_list(fitname, session_index)
        path = file_list[0]
        with h5py.File(path, 'r') as f:
            column_names = list(f['results'].keys())
        results_as_dict = {k: [] for k in column_names}

        for path in file_list:
            with h5py.File(path, 'r') as f:
                for k in f['results']:
                    results_as_dict[k].append(np.array(f['results'][k]))
        results_as_dict = {k: np.hstack(results_as_dict[k]) for k in results_as_dict}
        df = pd.DataFrame(results_as_dict)
        return df

    def _prepare_spectra_lookup(self, fitname):
        """ Creates an index on that spans all the sessions so that users can
        provide a single row ID rather than a session-row pair """
        for session_index in self.session_list[fitname]:
            file_list = self._get_file_list(fitname, session_index)
            if self.verbose: print('creating directory for %d files in session index %d' % (len(file_list), session_index))
            self.selected_files[fitname][session_index] = {}
            for filename in file_list:
                with h5py.File(filename, 'r') as h5_file:
                    row_id = h5_file['results']['row_id'][:].astype(np.int64)
                    min_, max_ = row_id[0], row_id[-1]
                    self.all_row_ids[fitname] = np.append(self.all_row_ids[fitname], row_id)
                    self.session_array[fitname] = np.append(self.session_array[fitname], np.ones(row_id.shape, dtype=np.int64)*session_index)
                    self.selected_files[fitname][session_index][range(min_, max_+1)] = filename
        self.unique_spectrum_id[fitname] = np.arange(len(self.all_row_ids[fitname]), dtype=np.int64)

    def get_spectra_row(self, fitname, row, pull_results=False):
        """
        Accesses a spectrum dataset from a log.

        Arguments:
            fitname (str): Fit name in log file
            row (int): Ordinal index of the row within a set of sessions
            pull_results(bool): Also return the fit results alongside the spectrum

        Returns:
            tuple: Tuple of dictionaries of 1D numpy arrays ``spectrum``, and ``results``

        """
        results = {}
        spectrum = {}
        target_file = None
        selected_sessions = set(self.selected_files[fitname].keys())
        current_sessions = set(self.session_list[fitname])
        if not self.selected_files[fitname] or selected_sessions != current_sessions:
            # Create an index so that users can index rows start from zero
            self._prepare_spectra_lookup(fitname)

        max_row = self.unique_spectrum_id[fitname][-1]
        if row > max_row:
            raise ValueError(f'Spectrum row ID {row} exceeds range of data: {max_row}')

        row_id = self.all_row_ids[fitname][row]
        session_id = self.session_array[fitname][row]
        if self.verbose:
            if row % 20 == 0:
                # print(f'Getting spectrum {row} from session {session_id} and row_id {row_id}')
                print('Getting spectrum %s / %s' % (row, max_row))
        for range_ in self.selected_files[fitname][session_id]:
            if row_id in range_:
                target_file = self.selected_files[fitname][session_id][range_]
                break

        if target_file is not None:
            # Target acquired
            with h5py.File(target_file, 'r') as h5_file:
                for key, value in h5_file['spectra'][f'{row_id:07}'].items():
                    spectrum[key] = value[:]
                if pull_results:
                    results = {}
                    # Thanks to the `row_id` key that we stored during logging, we can map back
                    # into the results row, which is a different index
                    result_index = np.where(np.array(h5_file['results']['row_id']) == row_id)[0][0]
                    for key, value in h5_file['results'].items():
                        results[key] = value[result_index]
        return spectrum, results, max_row



